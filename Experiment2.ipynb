{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective : WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of a simple Multi-Layer Perceptron (MLP) network with one hidden layer that can learn the XOR Boolean function. We'll use the step function (which outputs 0 or 1) as the activation function and employ the backpropagation algorithm to train the network.\n",
    "\n",
    "The network will have:\n",
    "\n",
    "2 input neurons (to represent the two inputs for XOR).\n",
    "4 neurons in the hidden layer.\n",
    "1 output neuron to predict the XOR result.\n",
    "The step function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step Activation Function:\n",
    "\n",
    "We use the step function for both the hidden and output layers. It outputs 1 if the input is >= 0, otherwise, it outputs 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step function (activation function)\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step_derivative(x): This is the derivative of the step function.\n",
    "For the step function, its derivative is always 1 because the function is piecewise constant. We don't really use it here (for the step function, the derivative isn't very meaningful, but we define it for structure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of step function (not used directly in backpropagation but kept for structure)\n",
    "def step_derivative(x):\n",
    "    return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR inputs and outputs\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 XOR input pairs\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR outputs\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Parameters for the neural network\n",
    "input_size = 2  # 2 inputs\n",
    "hidden_size = 4  # 4 neurons in the hidden layer\n",
    "output_size = 1  # 1 output\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "w1 = np.random.rand(input_size, hidden_size)  # Weights for input to hidden layer\n",
    "b1 = np.zeros((1, hidden_size))  # Biases for hidden layer\n",
    "w2 = np.random.rand(hidden_size, output_size)  # Weights for hidden to output layer\n",
    "b2 = np.zeros((1, output_size))  # Biases for output layer\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "epochs = 10000  # Number of training iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size: The number of inputs (2 in the XOR problem).\n",
    "\n",
    "hidden_size: The number of neurons in the hidden layer (4 neurons).\n",
    "\n",
    "output_size: The number of outputs (1 output for XOR).\n",
    "\n",
    "w1: Randomly initialized weights connecting the input layer to the hidden layer. The shape is (2, 4) because there are 2 inputs and 4 neurons in the hidden layer.\n",
    "\n",
    "b1: Bias values for the hidden layer. Initialized to zeros with shape (1, 4).\n",
    "\n",
    "w2: Randomly initialized weights connecting the hidden layer to the output layer. The shape is (4, 1) because there are 4 neurons in the hidden layer and 1 output neuron.\n",
    "\n",
    "b2: Bias values for the output layer. Initialized to zeros with shape (1, 1).\n",
    "\n",
    "learning_rate: The learning rate controls how much the weights and biases are adjusted during each training iteration. A typical value is between 0.01 and 0.1.\n",
    "\n",
    "epochs: The number of times the entire dataset will be passed through the network during training. In this case, it's set to 10,000 iterations to ensure enough learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 0.5\n",
      "Epoch 1000, Error: 0.5\n",
      "Epoch 2000, Error: 0.5\n",
      "Epoch 3000, Error: 0.5\n",
      "Epoch 4000, Error: 0.5\n",
      "Epoch 5000, Error: 0.5\n",
      "Epoch 6000, Error: 0.5\n",
      "Epoch 7000, Error: 0.5\n",
      "Epoch 8000, Error: 0.5\n",
      "Epoch 9000, Error: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Training the MLP network\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    hidden_input = np.dot(X, w1) + b1  # Input to the hidden layer\n",
    "    hidden_output = step_function(hidden_input)  # Output of hidden layer (after activation)\n",
    "\n",
    "    output_input = np.dot(hidden_output, w2) + b2  # Input to output layer\n",
    "    output = step_function(output_input)  # Final output (after activation)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Compute the error at the output layer\n",
    "    output_error = y - output\n",
    "\n",
    "    # Gradients for output layer\n",
    "    output_delta = output_error * step_derivative(output_input)\n",
    "\n",
    "    # Compute the error at the hidden layer\n",
    "    hidden_error = output_delta.dot(w2.T)\n",
    "    hidden_delta = hidden_error * step_derivative(hidden_input)\n",
    "\n",
    "    # Update weights and biases using the gradients\n",
    "    w2 += hidden_output.T.dot(output_delta) * learning_rate\n",
    "    b2 += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    w1 += X.T.dot(hidden_delta) * learning_rate\n",
    "    b1 += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Optionally print the error every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        error = np.mean(np.abs(output_error))\n",
    "        print(f'Epoch {epoch}, Error: {error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation:\n",
    "\n",
    "hidden_input: The weighted sum of the inputs for the hidden layer. This is calculated by performing matrix multiplication (np.dot(X, w1)) and adding the bias (b1).\n",
    "\n",
    "The shape of hidden_input is (4, 4) since there are 4 data points and 4 hidden neurons.\n",
    "\n",
    "hidden_output: The output of the hidden layer, which is obtained by applying the step_function (activation function) to hidden_input.\n",
    "\n",
    "output_input: The weighted sum of the inputs to the output layer. This is calculated by performing matrix multiplication (np.dot(hidden_output, w2)) and adding the bias (b2).\n",
    "\n",
    "The shape of output_input is (4, 1) since there are 4 data points and 1 output neuron.\n",
    "\n",
    "output: The final output of the network, which is obtained by applying the step_function to output_input.\n",
    "\n",
    "\n",
    "Backpropagation (Weight Updates):\n",
    "\n",
    "Compute gradients (d_output and d_hidden) for adjusting weights.\n",
    "\n",
    "Update W1, W2, b1, and b2 using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final predictions after training:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# After training, check the predictions\n",
    "print(\"\\nFinal predictions after training:\")\n",
    "final_output = step_function(np.dot(step_function(np.dot(X, w1) + b1), w2) + b2)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
