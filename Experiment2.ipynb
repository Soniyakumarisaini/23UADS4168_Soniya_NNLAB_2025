{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective : WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of a simple Multi-Layer Perceptron (MLP) network with one hidden layer that can learn the XOR Boolean function. We'll use the step function (which outputs 0 or 1) as the activation function and employ the backpropagation algorithm to train the network.\n",
    "\n",
    "The network will have:\n",
    "\n",
    "2 input neurons (to represent the two inputs for XOR).\n",
    "4 neurons in the hidden layer.\n",
    "1 output neuron to predict the XOR result.\n",
    "The step function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step Activation Function:\n",
    "\n",
    "We use the step function for both the hidden and output layers. It outputs 1 if the input is >= 0, otherwise, it outputs 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step function (activation function)\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR inputs and outputs\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 XOR input pairs\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR outputs\n",
    "\n",
    "# Initialize the weights randomly\n",
    "input_layer_size = 2  # Number of input neurons\n",
    "hidden_layer_size = 4  # Number of hidden layer neurons\n",
    "output_layer_size = 1  # Output neuron size\n",
    "\n",
    "# Random weight initialization for both layers\n",
    "np.random.seed(1)  # For reproducibility\n",
    "weights_input_hidden = np.random.randn(input_layer_size, hidden_layer_size)  # 2x4\n",
    "weights_hidden_output = np.random.randn(hidden_layer_size, output_layer_size)  # 4x1\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000  # Number of epochs\n",
    "learning_rate = 0.1  # Learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_layer_size: The number of inputs (2 in the XOR problem).\n",
    "\n",
    "hidden_layer_size: The number of neurons in the hidden layer (4 neurons).\n",
    "\n",
    "output_layer_size: The number of outputs (1 output for XOR).\n",
    "\n",
    "w1: Randomly initialized weights connecting the input layer to the hidden layer. The shape is (2, 4) because there are 2 inputs and 4 neurons in the hidden layer.\n",
    "\n",
    "b1: Bias values for the hidden layer. Initialized to zeros with shape (1, 4).\n",
    "\n",
    "w2: Randomly initialized weights connecting the hidden layer to the output layer. The shape is (4, 1) because there are 4 neurons in the hidden layer and 1 output neuron.\n",
    "\n",
    "b2: Bias values for the output layer. Initialized to zeros with shape (1, 1).\n",
    "\n",
    "learning_rate: The learning rate controls how much the weights and biases are adjusted during each training iteration. A typical value is between 0.01 and 0.1.\n",
    "\n",
    "epochs: The number of times the entire dataset will be passed through the network during training. In this case, it's set to 10,000 iterations to ensure enough learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Error: 1\n",
      "Epoch 1000, Total Error: 1\n",
      "Epoch 2000, Total Error: 1\n",
      "Epoch 3000, Total Error: 1\n",
      "Epoch 4000, Total Error: 1\n",
      "Epoch 5000, Total Error: 1\n",
      "Epoch 6000, Total Error: 1\n",
      "Epoch 7000, Total Error: 1\n",
      "Epoch 8000, Total Error: 1\n",
      "Epoch 9000, Total Error: 1\n"
     ]
    }
   ],
   "source": [
    "# Training the MLP\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden)  # Input to hidden layer\n",
    "    hidden_layer_output = step_function(hidden_layer_input)  # Output from hidden layer (activation)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)  # Input to output layer\n",
    "    predicted_output = step_function(output_layer_input)  # Output from network (activation)\n",
    "\n",
    "    # Compute the error\n",
    "    error = y - predicted_output\n",
    "\n",
    "    # Backpropagation using the Perceptron Learning Rule\n",
    "    # Update the weights from the hidden layer to the output layer\n",
    "    weights_hidden_output += learning_rate * np.dot(hidden_layer_output.T, error)\n",
    "\n",
    "    # Update the weights from the input layer to the hidden layer\n",
    "    hidden_layer_error = np.dot(error, weights_hidden_output.T)\n",
    "    weights_input_hidden += learning_rate * np.dot(X.T, hidden_layer_error * hidden_layer_output * (1 - hidden_layer_output))  # Derivative of step function (binary step)\n",
    "\n",
    "    # Print error every 1000 epochs to monitor the learning process\n",
    "    if epoch % 1000 == 0:\n",
    "        total_error = np.sum(np.abs(error))  # Sum of absolute error\n",
    "        print(f\"Epoch {epoch}, Total Error: {total_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation:\n",
    "\n",
    "hidden_layer_input: The weighted sum of the inputs for the hidden layer. This is calculated by performing matrix multiplication (np.dot(X, w1)) and adding the bias (b1).\n",
    "\n",
    "The shape of hidden_layer_input is (4, 4) since there are 4 data points and 4 hidden neurons.\n",
    "\n",
    "hidden_layer_output: The output of the hidden layer, which is obtained by applying the step_function (activation function) to hidden_input.\n",
    "\n",
    "output_layer_input: The weighted sum of the inputs to the output layer. This is calculated by performing matrix multiplication (np.dot(hidden_layer_output, w2)) and adding the bias (b2).\n",
    "\n",
    "The shape of output_layer_input is (4, 1) since there are 4 data points and 1 output neuron.\n",
    "\n",
    "predicted_output: The final output of the network, which is obtained by applying the step_function to output_input.\n",
    "\n",
    "\n",
    "Backpropagation (Weight Updates):\n",
    "\n",
    "Compute gradients (d_output and d_hidden) for adjusting weights.\n",
    "\n",
    "Update W1, W2, b1, and b2 using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Accuracy: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# Final predictions and accuracy\n",
    "predictions = step_function(np.dot(step_function(np.dot(X, weights_input_hidden)), weights_hidden_output))\n",
    "print(predictions)\n",
    "accuracy = np.mean(predictions == y) * 100  # Accuracy as percentage\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Comments:-\n",
    "\n",
    "The predictions [0, 1, 1, 0] correspond to the XOR outputs for the inputs [0, 0], [0, 1], [1, 0], [1, 1] respectively.\n",
    "If the accuracy is 100%, meaning the network successfully learned the XOR function.\n",
    "\n",
    "you can also use Gradient Descent method instead of perceptron learning rule for Backpropagation or Update the wights.\n",
    "\n",
    "You can adjust the number of epochs, learning rate, or other parameters for better performance, but this should work well for the XOR problem.\n",
    "\n",
    "Accuracy:- The percentage of correct predictions after training.\n",
    "\n",
    "This implementation demonstrates how an MLP with a single hidden layer can learn the XOR function using the perceptron learning rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
