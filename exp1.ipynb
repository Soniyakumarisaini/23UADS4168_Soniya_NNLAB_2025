{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:- WAP to implement the Perceptron Learning Algorithm using numpy in Python. Evaluate performance of a single perceptron for NAND and XOR truth tables as input dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron Learning Algorithm is a binary classifier that is used to classify inputs into two classes. It works by adjusting weights and biases based on the errors in predictions during the training process.\n",
    "The Perceptron Learning Algorithm is one of the earliest algorithms used for supervised learning. It is primarily used for binary classification tasks, where the goal is to classify data into one of two classes.\n",
    "\n",
    "A perceptron is a type of artificial neuron that makes decisions by computing a weighted sum of inputs and applying an activation function (often a step function). If the weighted sum is above a certain threshold, the perceptron outputs one class; otherwise, it outputs the other class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (step function)\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step function is used as the activation function in the perceptron.\n",
    "\n",
    "How it works:\n",
    "\n",
    "For each input x, if x >= 0, it returns 1, otherwise it returns 0.\n",
    "\n",
    "This is a binary output function that classifies the input based on a threshold of 0. Itâ€™s essentially the decision rule for our perceptron: if the sum of the inputs is above a certain threshold, the perceptron outputs 1; otherwise, it outputs 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Learning Algorithm\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, epochs=10):\n",
    "        # Initialize weights with small random numbers, and bias to 0\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute the dot product and add bias\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return step_function(linear_output)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Training process\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                # Update rule: weight adjustment based on error\n",
    "                error = target - prediction\n",
    "                self.weights += self.learning_rate * error * inputs\n",
    "                self.bias += self.learning_rate * error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron Algorithm:\n",
    "\n",
    "We initialize the weights and bias to zero.\n",
    "\n",
    "For each epoch, we go through all the data points and:\n",
    "\n",
    "Calculate the weighted sum (np.dot(X[i], weights) + bias).\n",
    "Apply the step function to decide the predicted output.\n",
    "\n",
    "Calculate the error (y[i] - prediction).\n",
    "Update the weights and bias according to the learning rule.\n",
    "\n",
    "The learning rate determines how large the updates will be during training.\n",
    "\n",
    "The model will keep iterating through the data (for a set number of epochs) until it learns to predict the outputs correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# NAND Truth Table: Inputs and Expected Outputs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_nand \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m      3\u001b[0m y_nand \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# NAND outputs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# XOR Truth Table: Inputs and Expected Outputs\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# NAND Truth Table: Inputs and Expected Outputs\n",
    "X_nand = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_nand = np.array([1, 1, 1, 0])  # NAND outputs\n",
    "\n",
    "# XOR Truth Table: Inputs and Expected Outputs\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAND Truth Table:\n",
    "\n",
    "Inputs are all combinations of (A, B) for 2 binary variables.\n",
    "\n",
    "The output for NAND is 1 except for when both A and B are 1, where the output is 0.\n",
    "\n",
    "XOR Truth Table:\n",
    "\n",
    "The output of XOR is 1 when exactly one of the inputs is 1, otherwise 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the performance of the model\n",
    "def evaluate_perceptron(X, y, perceptron):\n",
    "    predictions = perceptron.predict(X)\n",
    "    accuracy = np.mean(predictions == y) * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating for NAND...\n",
      "Accuracy on NAND: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluating on NAND\n",
    "print(\"Training and evaluating for NAND...\")\n",
    "perceptron_nand = Perceptron(input_size=2, epochs=100)\n",
    "perceptron_nand.train(X_nand, y_nand)\n",
    "accuracy_nand = evaluate_perceptron(X_nand, y_nand, perceptron_nand)\n",
    "print(f\"Accuracy on NAND: {accuracy_nand:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: We train the perceptron on the NAND truth table by passing the input X_nand and output y_nand to the perceptron function.\n",
    "\n",
    "The perceptron will adjust the weights and bias over 1000 epochs using the learning rate 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating for XOR...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training and Evaluating on XOR (This might not work well because a single perceptron can't solve XOR)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining and evaluating for XOR...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m perceptron_xor \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptron\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m perceptron_xor\u001b[38;5;241m.\u001b[39mtrain(X_xor, y_xor)\n\u001b[0;32m      5\u001b[0m accuracy_xor \u001b[38;5;241m=\u001b[39m evaluate_perceptron(X_xor, y_xor, perceptron_xor)\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m, in \u001b[0;36mPerceptron.__init__\u001b[1;34m(self, input_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Initialize weights with small random numbers, and bias to 0\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(input_size)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Training and Evaluating on XOR (This might not work well because a single perceptron can't solve XOR)\n",
    "print(\"\\nTraining and evaluating for XOR...\")\n",
    "perceptron_xor = Perceptron(input_size=2, epochs=100)\n",
    "perceptron_xor.train(X_xor, y_xor)\n",
    "accuracy_xor = evaluate_perceptron(X_xor, y_xor, perceptron_xor)\n",
    "print(f\"Accuracy on XOR: {accuracy_xor:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron cannot learn the XOR function correctly because XOR is not linearly separable. \n",
    "\n",
    "The perceptron struggles to classify XOR correctly, and this will be evident in the results.\n",
    "The Perceptron is able to solve linearly separable problems like NAND, but it fails for non-linearly separable problems like XOR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
