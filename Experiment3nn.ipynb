{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:-  WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.\n",
    "\n",
    "Description of the model:-  This is a three-layer neural network implemented using TensorFlow (without Keras) for classifying handwritten digits from the MNIST dataset.\n",
    "\n",
    "The model consists:-\n",
    "-> Input Layer (784 neurons\\nodes):- Takes flattened 28x28 pixel images.\n",
    "-> Hidden Layer 1 (128 neurons\\nodes):- Sigmoid activation function.\n",
    "-> Hidden Layer 2 (64 neurons\\nodes):- Sigmoid activation function.\n",
    "-> Output Layer (10 neurons\\nodes):- Uses softmax activation for multi-class classification digits (0-9).\n",
    "-> Loss Function:- softmax Categorical cross-entropy.\n",
    "-> Optimizer:-  Adam Optimizer.\n",
    "-> Epochs:- 10\n",
    "-> Learning Rate:- 0.01\n",
    "-> Batch size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize (0 to 1)\n",
    "    image = tf.reshape(image, [-1])  # Flatten (28x28 → 784)\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode labels\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# tf.optimizers.Adam: Initializes the Adam optimizer, which is an advanced gradient descent method that adapts the learning rate during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape(): Tracks the operations performed on the tensors in the forward pass to calculate gradients during backpropagation.\n",
    "\n",
    "tape.gradient(loss, ...): Computes the gradients of the loss with respect to the model parameters (weights and biases).\n",
    "\n",
    "optimizer.apply_gradients: Applies the gradients to update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 53.7979\n",
      "Epoch 2, Loss: 35.1091\n",
      "Epoch 3, Loss: 27.0394\n",
      "Epoch 4, Loss: 22.7045\n",
      "Epoch 5, Loss: 22.5453\n",
      "Epoch 6, Loss: 20.0231\n",
      "Epoch 7, Loss: 19.7101\n",
      "Epoch 8, Loss: 15.4758\n",
      "Epoch 9, Loss: 16.7661\n",
      "Epoch 10, Loss: 16.9674\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch loop: The model is trained for 10 epochs. In each epoch:\n",
    "    \n",
    "The model processes all batches of training data.\n",
    "\n",
    "The train_step function is called to perform a forward pass, calculate the loss, and apply gradients.\n",
    "\n",
    "The loss is accumulated for the epoch and printed at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy (Adam): 0.9940\n",
      "Final Test Accuracy (Adam): 0.9761\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (Adam): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (Adam): {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of Code:\n",
    "\n",
    "Load Data:- Loads the MNIST dataset (a collection of handwritten digits) using tensorflow_datasets.\n",
    "\n",
    "PreProcess Data:- preprocesses the MNIST dataset by normalizing the images , flattening them, and convert labels to one-hot encoding. \n",
    "\n",
    "Initialize Model Parameters:- \n",
    "    Weights (W1, W2, W3) initialized with  random values and biases (b1, b2, b3) initialized as zeros.\n",
    "\n",
    "Feed-Forward Propagation:-\n",
    "    hidden_Layer1 = sigmoid(X * W1 + b1).\n",
    "    hidden_layer2  = sigmoid(a1 * W2 + b2).\n",
    "    Output Layer:-  logits = softmax(a2 * W3 + b3).\n",
    "\n",
    "Loss Calculation:-\n",
    "   tf.nn.softmax_cross_entropy_with_logits: Computes the softmax cross-entropy loss, which is appropriate for multi-class classification problems like MNIST.\n",
    "   Softmax: Converts the raw logits into probability distributions.\n",
    "   Cross-Entropy: Measures the difference between the predicted probabilities and the actual labels (one-hot encoded).\n",
    "\n",
    "Backpropagation & Optimization\n",
    "   Uses AdamOptimizer() and Updates weights and biases.\n",
    "\n",
    "Training:- Iterates through 10 epochs with batch size = 100.\n",
    "           Prints training loss & accuracy.\n",
    "           The model is trained using the Adam optimizer and the softmax cross-entropy loss function. The model’s parameters are updated during each training step.\n",
    "\n",
    "Evaluation:-  After training, the model’s accuracy on the training and test datasets is computed and printed.\n",
    "\n",
    "This code implements a basic neural network from scratch using TensorFlow and trains it to classify handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MYy Comments: -\n",
    "\n",
    "Use Relu Activation function or Variants of relu for Faster Convergence than sigmoid.Set low Learning rate when you use relu. \n",
    "Use Softmax in output layer for multi-class classification.\n",
    "\n",
    "Use Sparse categorical cross entropy instead of using categorical cross entropy loss function because no need to do one hot encoding in Sparse categorical cross entropy.\n",
    "\n",
    "Adam optimizer's learning rate must be low.It works better with greater batch size -Faster Compared to SGD.\n",
    "\n",
    "Training Accuracy: 0.9940 \n",
    "\n",
    "Test Accuracy: 0.9761"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
