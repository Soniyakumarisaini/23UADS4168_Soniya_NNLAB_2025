{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJECTIVE : WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.\n",
    "\n",
    "Model Description: Architecture: Input Layer (784 neurons) → Takes flattened 28×28 grayscale images. Hidden Layer 1 (128 neurons) → Sigmoid activation. Hidden Layer 2 (64 neurons) → Sigmoid activation. Output Layer (10 neurons) → Produces logits, passed to Softmax for classification. Hyperparameters: Loss Function: Softmax Cross-Entropy Optimizer: Adam Learning Rate: 0.01 Batch Size: 100 Epochs: 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the MNIST dataset (a collection of handwritten digits).\n",
    "\n",
    "as_supervised=True: Returns the dataset as pairs of (image, label).\n",
    "\n",
    "with_info=True: Also returns metadata (info) about the dataset, including the dataset’s shape, number of classes, etc.\n",
    "\n",
    "\n",
    "Defines the batch size for training and testing. In each training step, 100 samples will be processed in parallel (this helps improve computational efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize (0 to 1)\n",
    "    image = tf.reshape(image, [-1])  # Flatten (28x28 → 784)\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode labels\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image = tf.cast(image, tf.float32) / 255.0:\n",
    "\n",
    "Converts the image to float32 type and normalizes the pixel values to the range [0, 1] (since pixel values in MNIST are originally between 0 and 255).\n",
    "image = tf.reshape(image, [-1]):\n",
    "\n",
    "Flattens the 28x28 image into a vector of length 784 (28 * 28 = 784), which is a common practice when feeding images into fully connected layers.\n",
    "\n",
    "For example, if you have a tensor of shape (28, 28), which represents an image of size 28x28 (28 rows and 28 columns of pixel values), and you want to reshape it to a one-dimensional vector, you don’t necessarily need to explicitly specify the new dimension size (in this case, 784). You can use -1 to allow TensorFlow to automatically compute the correct value for that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset.map(preprocess): Applies the preprocess function to each image and label pair in the train_dataset.\n",
    "\n",
    ".shuffle(10000): Shuffles the training data before batching. This ensures that the model doesn't memorize the order of the data, improving generalization.\n",
    "\n",
    ".batch(BATCH_SIZE): Divides the data into batches of size 100.\n",
    "\n",
    "For test dataset: The same preprocessing (map(preprocess)) is applied, but no shuffling is performed (since it's for evaluation), and the data is batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 10\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits: Computes the softmax cross-entropy loss, which is appropriate for multi-class classification problems like MNIST.\n",
    "\n",
    "Softmax: Converts the raw logits into probability distributions.\n",
    "\n",
    "Cross-Entropy: Measures the difference between the predicted probabilities and the actual labels (one-hot encoded).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.argmax(logits, axis=1): Finds the class with the highest probability in the output logits.\n",
    "\n",
    "tf.equal(...): Compares the predicted class with the actual class (the one-hot encoded labels).\n",
    "\n",
    "tf.reduce_sum(...): Counts the number of correct predictions in the batch.\n",
    "\n",
    "compute_accuracy: Iterates through the dataset and computes the fraction of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# tf.optimizers.Adam: Initializes the Adam optimizer, which is an advanced gradient descent method that adapts the learning rate during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape(): Tracks the operations performed on the tensors in the forward pass to calculate gradients during backpropagation.\n",
    "\n",
    "tape.gradient(loss, ...): Computes the gradients of the loss with respect to the model parameters (weights and biases).\n",
    "\n",
    "optimizer.apply_gradients: Applies the gradients to update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 170.0491\n",
      "Epoch 2, Loss: 67.4476\n",
      "Epoch 3, Loss: 50.1181\n",
      "Epoch 4, Loss: 39.9755\n",
      "Epoch 5, Loss: 35.3683\n",
      "Epoch 6, Loss: 31.6751\n",
      "Epoch 7, Loss: 26.0732\n",
      "Epoch 8, Loss: 24.7058\n",
      "Epoch 9, Loss: 23.7283\n",
      "Epoch 10, Loss: 24.6713\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch loop: The model is trained for 10 epochs. In each epoch:\n",
    "    \n",
    "The model processes all batches of training data.\n",
    "\n",
    "The train_step function is called to perform a forward pass, calculate the loss, and apply gradients.\n",
    "\n",
    "The loss is accumulated for the epoch and printed at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy (Adam): 0.9848\n",
      "Final Test Accuracy (Adam): 0.9696\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (Adam): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (Adam): {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Data Loading and Preprocessing: Loads and preprocesses the MNIST dataset using tensorflow_datasets, normalizing the images, flattening them, and one-hot encoding the labels.\n",
    "\n",
    "Model Definition: A 3-layer neural network with two hidden layers and an output layer.\n",
    "\n",
    "Training: The model is trained using the Adam optimizer and the softmax cross-entropy loss function. The model’s parameters are updated during each training step.\n",
    "\n",
    "Evaluation: After training, the model’s accuracy on the training and test datasets is computed and printed.\n",
    "\n",
    "This code implements a basic neural network from scratch using TensorFlow and trains it to classify handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MY COMMENTS: -\n",
    "\n",
    "Adam optimizer's learning rate must be low. -Use ReLU instead of Sigmoid. -It works better with greater batch size -Faster Compared to SGD\n",
    "\n",
    "Training Accuracy: 0.9906 \n",
    "\n",
    "Test Accuracy: 0.9738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
